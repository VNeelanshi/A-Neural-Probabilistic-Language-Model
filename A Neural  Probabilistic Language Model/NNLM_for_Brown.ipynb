{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "My NNLM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4aQIHYYJVZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath = 'brown_train.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq9WIOPwAnKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no228r1oJVZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "    \n",
        "def load(filepath, window_size, vocab_size=None):\n",
        "\n",
        "    words = []\n",
        "    with open(filepath, 'r', encoding='utf8') as file:\n",
        "        words = word_tokenize(file.readline())    \n",
        "\n",
        "    x_train, y_train = [], []\n",
        "    for i in range(len(words) - window_size + 1):\n",
        "        x_train.append(words[i: i + window_size - 1])\n",
        "        y_train.append(words[i +  window_size - 1])\n",
        "    \n",
        "    vocab = [word[0] for word in Counter(words).most_common(vocab_size)]\n",
        "    word2id = { vocab[i]: i for i in range(len(vocab)) }\n",
        "    \n",
        "    return np.array(x_train), np.array(y_train)[:,None], np.array(vocab), word2id\n",
        "\n",
        "def load_zh(filepath, window_size, vocab_size=None):\n",
        "\n",
        "    words = []\n",
        "    with open(filepath, 'r', encoding='utf8') as file:\n",
        "        for line in file:\n",
        "            words += word_tokenize(line.strip())\n",
        "        \n",
        "\n",
        "    x_train, y_train = [], []\n",
        "    for i in range(len(words) - window_size + 1):\n",
        "        x_train.append(words[i: i + window_size - 1])\n",
        "        y_train.append(words[i +  window_size - 1])\n",
        "    \n",
        "    vocab = [word[0] for word in Counter(words).most_common(vocab_size)]\n",
        "    word2id = { vocab[i]: i for i in range(len(vocab)) }\n",
        "    \n",
        "    return np.array(x_train), np.array(y_train)[:,None], np.array(vocab), word2id\n",
        "            \n",
        "def convert_to_id(x_train, y_train, vocab):\n",
        "    \n",
        "    word_to_id = {}\n",
        "    for i, vocab in enumerate(vocab):\n",
        "        word_to_id[vocab] = i\n",
        "        \n",
        "    for i in range(len(x_train)):\n",
        "        x_train[i] = [word_to_id[word] for word in x_train[i]]\n",
        "        y_train[i] = word_to_id[y_train[i][0]]\n",
        "        \n",
        "    return x_train.astype(int), y_train.astype(int)\n",
        "\n",
        "\n",
        "def next_batch(x_train, y_train, batch_size):\n",
        "    \n",
        "    num_batch = len(x_train) // batch_size + 1\n",
        "    for n in range(num_batch):        \n",
        "        offset = n * batch_size\n",
        "        x_batch = x_train[offset: offset + batch_size]\n",
        "        y_batch = y_train[offset: offset + batch_size]\n",
        "        \n",
        "        yield x_batch, y_batch\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4WGvLpMJVZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameters\n",
        "batch_size = 30\n",
        "window_size = 6\n",
        "vocab_size = None\n",
        "hidden_size = 50\n",
        "emb_dim = 60\n",
        "learning_rate = 0.3\n",
        "epoch_size = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Had-f-TQJwvI",
        "colab_type": "code",
        "outputId": "470f0214-edb4-4593-f73d-b55ccababea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEW5yqMxJVZu",
        "colab_type": "code",
        "outputId": "b887134b-0904-47a0-c536-bbb24e1fb3f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_raw, y_raw, vocab, word2id = load_zh(filepath, window_size, vocab_size)\n",
        "vocab_size = len(vocab)\n",
        "print('vocab_size: {}'.format(vocab_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size: 52945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbWHEqDRJjr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath_v = 'brown_valid.txt'\n",
        "x_raw_v, y_raw_v, vocab_v, word2id_v = load_zh(filepath_v, window_size, vocab_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqZ8qrbSJbon",
        "colab_type": "code",
        "outputId": "731c38af-b268-4e4a-fd6d-4da5c4955000",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['<', '>', 'the', ..., 'lurched', 'muddied', 'dogtrot'],\n",
              "      dtype='<U38')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdynygmUJVZx",
        "colab_type": "code",
        "outputId": "f257dccf-b4fc-474f-e819-1c02a8067f70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# integer representations of vocab\n",
        "x_train, y_train = convert_to_id(x_raw, y_raw, vocab)\n",
        "print('Length: {}'.format(len(x_train)))\n",
        "print('Number of batch: {}'.format(len(x_train) / batch_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length: 1321189\n",
            "Number of batch: 44039.63333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hfo81jKZJsSw",
        "colab_type": "code",
        "outputId": "dc16e956-a14a-49ed-dae2-7a9bff312826",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x_valid, y_valid = convert_to_id(x_raw_v, y_raw_v, vocab_v)\n",
        "print('Length: {}'.format(len(x_valid)))\n",
        "print('Number of batch: {}'.format(len(x_valid) / batch_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length: 175397\n",
            "Number of batch: 5846.566666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mP2MomyLbD0",
        "colab_type": "code",
        "outputId": "5445af9b-d80a-474f-d119-c376323cb7dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1321189, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfZ3NR_AJVZ0",
        "colab_type": "code",
        "outputId": "5bb9437d-0bdc-4116-f8f7-c757aa00369d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# import tensorflow as tf\n",
        "# %tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQkPaB6VJVZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Parameter Definition\n",
        "\n",
        "\n",
        "# Input && Output\n",
        "input_words = tf.placeholder(dtype=tf.int32, shape=(batch_size, window_size-1))\n",
        "output_word = tf.placeholder(dtype=tf.int32, shape=(batch_size, 1))\n",
        "\n",
        "\n",
        "# Word Features\n",
        "# word embedding matrix\n",
        "# truncated_normal randomly initializes a matrix of the given shape with values from the normal distribution\n",
        "C = tf.Variable(tf.truncated_normal(shape=(vocab_size, emb_dim), mean=-1, stddev=-1), name='word_embedding')\n",
        "\n",
        "\n",
        "# Hidden Layer Weight && Bias\n",
        "H = tf.Variable(tf.random_normal(shape=(hidden_size, (window_size - 1 ) * emb_dim)))\n",
        "d = tf.Variable(tf.random_normal(shape=(hidden_size, )))\n",
        "\n",
        "# Hidden-to-Output Weight && Bias\n",
        "U = tf.Variable(tf.random_normal(shape=(vocab_size, hidden_size)))\n",
        "b = tf.Variable(tf.random_normal(shape=(vocab_size, )))\n",
        "\n",
        "# Projection-to-Output Weight\n",
        "W = tf.Variable(tf.random_normal(shape=(vocab_size, (window_size - 1) * emb_dim)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSbKQ-12JVZ7",
        "colab_type": "code",
        "outputId": "d02b615e-bd6f-4965-dd64-98919c0bca59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# y = b + Wx + Utanh(d + Hx)\n",
        "\n",
        "# x = (C(w(t-1)), C(w(t-2), ..., C(w(t-n+1))), n == window_size\n",
        "with tf.name_scope('Projection_Layer'):\n",
        "  # get the actual embedding vectors from our batch inputs\n",
        "    x  = tf.nn.embedding_lookup(C, input_words) # (batch_size, window_size-1, emb_dim)\n",
        "    x  = tf.reshape(x, shape=(batch_size, (window_size - 1) * emb_dim))\n",
        "    \n",
        "with tf.name_scope('Hidden_Layer'):\n",
        "    Hx = tf.matmul(x, tf.transpose(H)) # (batch_size, hidden_size)\n",
        "    o  = tf.add(d, Hx) # (batch_size, hidden_size)\n",
        "    a  = tf.nn.tanh(o)  # (batch_size, hidden_size)\n",
        "     \n",
        "with tf.name_scope('Output_Layer'):\n",
        "    Ua = tf.matmul(a, tf.transpose(U)) # (batch_size, vocab_size)\n",
        "    Wx = tf.matmul(x, tf.transpose(W)) # (batch_size, vocab_size)\n",
        "    y  = tf.nn.softmax(tf.clip_by_value(tf.add(b, tf.add(Wx, Ua)), 0.0, 10)) # (batch_size, vocab_size)\n",
        "    #ppl = -1*tf.log(y)\n",
        "\n",
        "with tf.name_scope('Loss'):\n",
        "    onehot_tgt = tf.one_hot(tf.squeeze(output_word), vocab_size)  # (batch_size, vocab_size)\n",
        "    loss = -1 * tf.reduce_mean(tf.reduce_sum(tf.log(y) * onehot_tgt, 1)) # ä¹˜ -1 -> maximize loss\n",
        "   \n",
        "with tf.name_scope('Perplexity'):\n",
        "    ppl = tf.math.exp(loss)\n",
        "    #print(ppl)\n",
        "    \n",
        "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss) \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ0x6Zu7bYRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMHizXWCJVZ-",
        "colab_type": "code",
        "outputId": "c8c1f43c-67b7-452f-d64c-046ab7187f5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
        "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=True)) as sess:\n",
        "    # initializes all of those variables we declared in earlier cells!\n",
        "    initializer = tf.global_variables_initializer()\n",
        "    initializer.run()\n",
        "    \n",
        "    step = 0\n",
        "    avg_loss_t = 0\n",
        "    avg_loss_v = 0\n",
        "    loss_t = []\n",
        "    loss_v = []\n",
        "    saver.restore(sess, \"/content/gdrive/My Drive/nnlm/final_model.ckpt\")\n",
        "    for epoch in range(epoch_size):\n",
        "        print('epoch no ', epoch)\n",
        "        save_path = saver.save(sess, \"/content/gdrive/My Drive/nnlm/final_model.ckpt\")\n",
        "\n",
        "        for x_batch, y_batch in next_batch(x_train, y_train, batch_size):\n",
        "            # if the batch is smaller than it's supposed to be (i.e. at end of vocab), skip it\n",
        "            # TODO: change this to account for num_batches, not batch_size\n",
        "\n",
        "            if len(x_batch) != batch_size:\n",
        "                continue\n",
        "            # give TF the data to use for all of the calcs in previous cells\n",
        "            feed_dict = {input_words: x_batch, output_word: y_batch}\n",
        "            # here we tell TF to return the loss to us \n",
        "            fetches = [loss, optimizer]\n",
        "            # where the magic happens \n",
        "            Loss, _ = sess.run(fetches, feed_dict)\n",
        "            avg_loss_t += Loss\n",
        "            #ppl = Perplexity\n",
        "            if step % 1000 == 0:\n",
        "                print('Step {}, Loss: {}'.format(step, avg_loss_t / 1000))\n",
        "                #print('Perplexity: {}'.format(ppl))\n",
        "\n",
        "                for valid_x, valid_y in next_batch(x_valid, y_valid, batch_size):\n",
        "                  if len(valid_x) != batch_size:\n",
        "                    continue\n",
        "                  feed_dict = {input_words: valid_x, output_word: valid_y}\n",
        "                  fetches = [loss, optimizer]\n",
        "                  Loss, _ = sess.run(fetches, feed_dict)\n",
        "                  avg_loss_v += Loss\n",
        "                print('Step {}, Loss: {}'.format(step, avg_loss_v / 1000))\n",
        "                loss_t.append(avg_loss_t)\n",
        "                loss_v.append(avg_loss_v)\n",
        "                avg_loss_t = 0\n",
        "                avg_loss_v = 0\n",
        "                \n",
        "            step += 1\n",
        "        \n",
        "    print('Training Done.')\n",
        "    word_embedding = C.eval()    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/nnlm/final_model.ckpt\n",
            "epoch no  0\n",
            "Step 0, Loss: 0.008834031105041504\n",
            "Step 0, Loss: 15.906856119394302\n",
            "Step 1000, Loss: 6.618669257879257\n",
            "Step 1000, Loss: 15.888002531528473\n",
            "Step 2000, Loss: 6.350076826334\n",
            "Step 2000, Loss: 15.876499027490615\n",
            "Step 3000, Loss: 6.506515859127044\n",
            "Step 3000, Loss: 15.867314840316773\n",
            "Step 4000, Loss: 6.530137540340424\n",
            "Step 4000, Loss: 15.868576750040054\n",
            "Step 5000, Loss: 6.673533667325973\n",
            "Step 5000, Loss: 15.866191981315612\n",
            "Step 6000, Loss: 6.630266601800918\n",
            "Step 6000, Loss: 15.862811940550804\n",
            "Step 7000, Loss: 6.686098099470138\n",
            "Step 7000, Loss: 15.863385906457902\n",
            "Step 8000, Loss: 6.620896711587906\n",
            "Step 8000, Loss: 15.865423483967781\n",
            "Step 9000, Loss: 6.608205669164658\n",
            "Step 9000, Loss: 15.859942431688308\n",
            "Step 10000, Loss: 6.719606333017349\n",
            "Step 10000, Loss: 15.859451622128487\n",
            "Step 11000, Loss: 6.918381294727325\n",
            "Step 11000, Loss: 15.866500587821006\n",
            "Step 12000, Loss: 6.683464585781097\n",
            "Step 12000, Loss: 15.855675251483918\n",
            "Step 13000, Loss: 6.410189059019089\n",
            "Step 13000, Loss: 15.852957130789758\n",
            "Step 14000, Loss: 6.754088258743286\n",
            "Step 14000, Loss: 15.855825881958008\n",
            "Step 15000, Loss: 6.892557843923568\n",
            "Step 15000, Loss: 15.85167550599575\n",
            "Step 16000, Loss: 6.735761759519577\n",
            "Step 16000, Loss: 15.848379934668541\n",
            "Step 17000, Loss: 6.592890516519547\n",
            "Step 17000, Loss: 15.850471469521523\n",
            "Step 18000, Loss: 6.708233429431916\n",
            "Step 18000, Loss: 15.849889559864998\n",
            "Step 19000, Loss: 6.7474704060554505\n",
            "Step 19000, Loss: 15.851259464979172\n",
            "Step 20000, Loss: 6.620610757827759\n",
            "Step 20000, Loss: 15.854101803660393\n",
            "Step 21000, Loss: 6.7937136399745945\n",
            "Step 21000, Loss: 15.852576719760895\n",
            "Step 22000, Loss: 6.78866064620018\n",
            "Step 22000, Loss: 15.851939094543457\n",
            "Step 23000, Loss: 6.712465109109878\n",
            "Step 23000, Loss: 15.855953914523125\n",
            "Step 24000, Loss: 6.750093737125397\n",
            "Step 24000, Loss: 15.857804299116134\n",
            "Step 25000, Loss: 6.490551722764969\n",
            "Step 25000, Loss: 15.851596737146378\n",
            "Step 26000, Loss: 6.603972268104553\n",
            "Step 26000, Loss: 15.854546214580536\n",
            "Step 27000, Loss: 6.654262647151947\n",
            "Step 27000, Loss: 15.850362898111344\n",
            "Step 28000, Loss: 6.620806196689606\n",
            "Step 28000, Loss: 15.853134404540063\n",
            "Step 29000, Loss: 6.766616926908493\n",
            "Step 29000, Loss: 15.854991059184075\n",
            "Step 30000, Loss: 6.711637559175491\n",
            "Step 30000, Loss: 15.846842094421387\n",
            "Step 31000, Loss: 6.652965864181518\n",
            "Step 31000, Loss: 15.84795627450943\n",
            "Step 32000, Loss: 6.707491454601288\n",
            "Step 32000, Loss: 15.843980963468551\n",
            "Step 33000, Loss: 6.513158571839333\n",
            "Step 33000, Loss: 15.84417119526863\n",
            "Step 34000, Loss: 6.272352648854255\n",
            "Step 34000, Loss: 15.841625541210174\n",
            "Step 35000, Loss: 6.922485360622406\n",
            "Step 35000, Loss: 15.842617830038071\n",
            "Step 36000, Loss: 6.91335048365593\n",
            "Step 36000, Loss: 15.845781515836716\n",
            "Step 37000, Loss: 6.884957155704498\n",
            "Step 37000, Loss: 15.845379512906074\n",
            "Step 38000, Loss: 6.897016388893127\n",
            "Step 38000, Loss: 15.83979500079155\n",
            "Step 39000, Loss: 6.574668237686157\n",
            "Step 39000, Loss: 15.839779862642288\n",
            "Step 40000, Loss: 6.426292047023773\n",
            "Step 40000, Loss: 15.840654544949532\n",
            "Step 41000, Loss: 6.059613110303879\n",
            "Step 41000, Loss: 15.841309698224068\n",
            "Step 42000, Loss: 6.2914363741874695\n",
            "Step 42000, Loss: 15.840539097428321\n",
            "Step 43000, Loss: 5.900571766376495\n",
            "Step 43000, Loss: 15.842957804918289\n",
            "Step 44000, Loss: 5.981880566835404\n",
            "Step 44000, Loss: 15.847219594359398\n",
            "Training Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt9utADwkSL_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_t = np.array(loss_t)\n",
        "plt.plot(range(len(loss_t)), np.exp(loss_t/1000))\n",
        "loss_v = np.array(loss_v)\n",
        "plt.plot(range(len(loss_v)), np.exp(loss_v/1000))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeabkDBJiS3o",
        "colab_type": "code",
        "outputId": "24c0f44e-5f02-4411-8cfa-0a1bb814d53a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "filepath_t = 'brown_test.txt'\n",
        "x_raw_t, y_raw_t, vocab_t, word2id_t = load_zh(filepath_t, window_size, vocab_size)\n",
        "x_test, y_test = convert_to_id(x_raw_t, y_raw_t, vocab_t)\n",
        "print('Length: {}'.format(len(x_test)))\n",
        "print('Number of batch: {}'.format(len(x_test) / batch_size))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length: 189235\n",
            "Number of batch: 6307.833333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riw3EQg5qt3a",
        "colab_type": "code",
        "outputId": "c7a8ea34-0a4f-448e-bf8c-f0d99fe1609e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "avg_loss_test = 0\n",
        "loss_test = []\n",
        "ctr = 0\n",
        "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
        "\n",
        "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=True)) as sess:\n",
        "  saver.restore(sess, \"/content/gdrive/My Drive/nnlm/final_model.ckpt\")\n",
        "  step = 0\n",
        "  for test_x, test_y in next_batch(x_test, y_test, batch_size):\n",
        "    if len(test_x) != batch_size:\n",
        "        continue\n",
        "    feed_dict = {input_words: test_x, output_word: test_y}\n",
        "    fetches = [loss, optimizer]\n",
        "    # where the magic happens \n",
        "    Loss, _ = sess.run(fetches, feed_dict)\n",
        "    avg_loss_test += Loss\n",
        "    ctr += 1\n",
        "\n",
        "    if step % 1000 == 0:\n",
        "      print('Step {}, Loss: {}'.format(step, avg_loss_test / 1000))\n",
        "      loss_test.append(avg_loss_test/1000)\n",
        "      avg_loss_test = 0\n",
        "    step += 1\n",
        "#print(np.exp(avg_loss_test/ctr))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/nnlm/final_model.ckpt\n",
            "Step 0, Loss: 0.008966852188110351\n",
            "Step 1000, Loss: 9.015638908863068\n",
            "Step 2000, Loss: 8.387402516841888\n",
            "Step 3000, Loss: 8.1041927485466\n",
            "Step 4000, Loss: 7.9367685546875\n",
            "Step 5000, Loss: 7.894798601150513\n",
            "Step 6000, Loss: 7.910759675502777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld6NjpeQscOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_test = np.array(loss_test)\n",
        "plt.plot(range(len(loss_test)), np.exp(loss_test))\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL6ySmt4Kvwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.sum(np.exp(loss_test))/len(loss_test)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}